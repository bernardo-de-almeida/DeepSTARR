{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train DeepSTARR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used packages and their version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GPU environment \n",
    "\n",
    "# conda create --name DeepSTARR python=3.7 tensorflow-gpu=1.14.0 keras-gpu=2.2.4\n",
    "# conda activate DeepSTARR\n",
    "# conda install numpy=1.16.2 pandas=0.25.3 matplotlib=3.1.1 ipykernel=5.4.3\n",
    "# pip install git+git://github.com/AvantiShri/shap.git@master\n",
    "# pip install 'h5py<3.0.0'\n",
    "# pip install deeplift==0.6.13.0\n",
    "# pip install keras-tuner==1.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, InputLayer, Input\n",
    "from keras import models\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('Neural_Network_DNA_Demo/')\n",
    "from helper import IOHelper, SequenceHelper # from https://github.com/const-ae/Neural_Network_DNA_Demo\n",
    "\n",
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA files with DNA sequences of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Train.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Val.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Test.fa'\n",
    "\n",
    "# Files with developmental and housekeeping activity of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Train.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Val.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(set):\n",
    "    # Convert sequences to one-hot encoding matrix\n",
    "    file_seq = str(\"Sequences_\" + set + \".fa\")\n",
    "    input_fasta_data_A = IOHelper.get_fastas_from_file(file_seq, uppercase=True)\n",
    "\n",
    "    # get length of first sequence\n",
    "    sequence_length = len(input_fasta_data_A.sequence.iloc[0])\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    seq_matrix_A = SequenceHelper.do_one_hot_encoding(input_fasta_data_A.sequence, sequence_length,\n",
    "                                                      SequenceHelper.parse_alpha_to_seq)\n",
    "    print(seq_matrix_A.shape)\n",
    "    \n",
    "    X = np.nan_to_num(seq_matrix_A) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "    Activity = pd.read_table(\"Sequences_activity_\" + set + \".txt\")\n",
    "    Y_dev = Activity.Dev_log2_enrichment\n",
    "    Y_hk = Activity.Hk_log2_enrichment\n",
    "    Y = [Y_dev, Y_hk]\n",
    "    \n",
    "    print(set)\n",
    "\n",
    "    return input_fasta_data_A.sequence, seq_matrix_A, X_reshaped, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for train/val/test sets\n",
    "X_train_sequence, X_train_seq_matrix, X_train, Y_train = prepare_input(\"Train\")\n",
    "X_valid_sequence, X_valid_seq_matrix, X_valid, Y_valid = prepare_input(\"Val\")\n",
    "X_test_sequence, X_test_seq_matrix, X_test, Y_test = prepare_input(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DeepSTARR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional metrics\n",
    "from scipy.stats import spearmanr\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'epochs': 100,\n",
    "          'early_stop': 10,\n",
    "          'kernel_size1': 7,\n",
    "          'kernel_size2': 3,\n",
    "          'kernel_size3': 5,\n",
    "          'kernel_size4': 3,\n",
    "          'lr': 0.002,\n",
    "          'num_filters': 256,\n",
    "          'num_filters2': 60,\n",
    "          'num_filters3': 60,\n",
    "          'num_filters4': 120,\n",
    "          'n_conv_layer': 4,\n",
    "          'n_add_layer': 2,\n",
    "          'dropout_prob': 0.4,\n",
    "          'dense_neurons1': 256,\n",
    "          'dense_neurons2': 256,\n",
    "          'pad':'same'}\n",
    "\n",
    "def DeepSTARR(params=params):\n",
    "    \n",
    "    lr = params['lr']\n",
    "    dropout_prob = params['dropout_prob']\n",
    "    n_conv_layer = params['n_conv_layer']\n",
    "    n_add_layer = params['n_add_layer']\n",
    "    \n",
    "    # body\n",
    "    input = kl.Input(shape=(249, 4))\n",
    "    x = kl.Conv1D(params['num_filters'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1st')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, n_conv_layer):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    for i in range(0, n_add_layer):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "    bottleneck = x\n",
    "    \n",
    "    # heads per task (developmental and housekeeping enhancer activities)\n",
    "    tasks = ['Dev', 'Hk']\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation='linear', name=str('Dense_' + task))(bottleneck))\n",
    "\n",
    "    model = keras.models.Model([input], outputs)\n",
    "    model.compile(keras.optimizers.Adam(lr=lr),\n",
    "                  loss=['mse', 'mse'], # loss\n",
    "                  loss_weights=[1, 1], # loss weigths to balance\n",
    "                  metrics=[Spearman]) # additional track metric\n",
    "\n",
    "    return model, params\n",
    "\n",
    "DeepSTARR()[0].summary()\n",
    "DeepSTARR()[1] # dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DeepSTARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(selected_model, X_train, Y_train, X_valid, Y_valid, params):\n",
    "\n",
    "    my_history=selected_model.fit(X_train, Y_train,\n",
    "                                  validation_data=(X_valid, Y_valid),\n",
    "                                  batch_size=params['batch_size'], epochs=params['epochs'],\n",
    "                                  callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True),\n",
    "                                             History()])\n",
    "    \n",
    "    return selected_model, my_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model, main_params = DeepSTARR()\n",
    "main_model, my_history = train(main_model, X_train, Y_train, X_valid, Y_valid, main_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance: mean squared error (MSE) and Pearson (PCC) and Spearman (SCC) correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(X, Y, set, task):\n",
    "    pred = main_model.predict(X, batch_size=main_params['batch_size'])\n",
    "    if task ==\"Dev\":\n",
    "        i=0\n",
    "    if task ==\"Hk\":\n",
    "        i=1\n",
    "    print(set + ' MSE ' + task + ' = ' + str(\"{0:0.2f}\".format(mean_squared_error(Y, pred[i].squeeze()))))\n",
    "    print(set + ' PCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.pearsonr(Y, pred[i].squeeze())[0])))\n",
    "    print(set + ' SCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.spearmanr(Y, pred[i].squeeze())[0])))\n",
    "    \n",
    "# run for each set and enhancer type\n",
    "summary_statistics(X_train, Y_train[0], \"train\", \"Dev\")\n",
    "summary_statistics(X_train, Y_train[1], \"train\", \"Hk\")\n",
    "summary_statistics(X_valid, Y_valid[0], \"validation\", \"Dev\")\n",
    "summary_statistics(X_valid, Y_valid[1], \"validation\", \"Hk\")\n",
    "summary_statistics(X_test, Y_test[0], \"test\", \"Dev\")\n",
    "summary_statistics(X_test, Y_test[1], \"test\", \"Hk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"DeepSTARR\"\n",
    "\n",
    "model_json = main_model.to_json()\n",
    "with open('Model_' + model_name + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "main_model.save_weights('Model_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning_conda_env_gpu",
   "language": "python",
   "name": "deeplearning_conda_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
